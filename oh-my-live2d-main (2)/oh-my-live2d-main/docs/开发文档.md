# Oh-My-Live2D 开发文档

## 一、项目概述

### 1.1 项目简介

Oh-My-Live2D 是一个基于 Web 技术的实时交互式虚拟形象聊天系统，集成了 Live2D 动态角色模型、语音识别、语音合成、音频评分和智能对话等多项前沿技术。该项目采用前后端分离的架构设计，前端使用原生 HTML、CSS 和 JavaScript 构建，后端基于 Python FastAPI 框架实现RESTful API 和 WebSocket 实时通信服务。系统的核心设计理念是为用户提供一个自然、直观的语音交互体验，用户可以通过语音输入与虚拟形象进行对话，系统能够智能识别语音内容、理解语义意图、生成回复并通过 TTS 技术进行语音输出，同时支持对用户语音进行专业评分和演唱技巧分析。

项目的技术栈涵盖了多个领域的前沿技术方案。在前端展示层，项目使用了 Live2D Web SDK 来渲染动态的二次元角色模型，配合精心设计的 CSS3 动画效果和响应式布局，确保了良好的视觉效果和交互体验。在语音处理层，项目集成了 DashScope 阿里云百炼平台的 ASR 和 TTS 服务，支持高精度的中英文语音识别和自然流畅的语音合成。在音频分析层，项目开发了基于深度学习的 CAM-S 音频评分系统，使用 CAMPPlus 神经网络模型对用户的音准、节奏、音色、情感和完整性进行多维度评分。此外，项目还接入了通义千问大语言模型，为用户提供智能化的对话服务，使整个系统具备了理解上下文、生成连贯回复的能力。

### 1.2 核心功能特性

本项目实现了以下核心功能模块，各模块之间通过标准化的 API 接口进行数据交换，形成了一套完整的语音交互工作流程：

**实时语音识别功能**允许用户通过麦克风输入语音内容，系统能够实时将语音转换为文字。该功能基于 DashScope 平台的 paraformer-v1 模型，支持中文普通话和英文的混合识别，识别准确率高、延迟低。系统采用 Web Audio API 录制音频数据，转换为 PCM 格式后通过 base64 编码传输给后端服务进行处理。整个识别过程采用了流式处理机制，边录制边识别，用户无需等待录制完成即可看到实时的识别结果。

**智能对话回复功能**接入了通义千问大语言模型，能够理解用户输入的文本内容并生成智能回复。系统支持多轮对话上下文管理，能够记住对话历史并根据上下文生成连贯的回复内容。回复生成过程采用了异步处理机制，避免阻塞前端界面的响应。同时，系统支持对回复文本进行情绪分析，根据不同的对话场景调整回复的语气和风格，提供更加自然和人性化的交互体验。

**语音合成播放功能**使用 DashScope 的语音合成服务，将生成的文本回复转换为自然流畅的语音进行播放。系统支持多种语音角色选择，包括中文男声、女声以及多种预设风格。TTS 服务采用 WebSocket 协议进行通信，支持流式音频传输，能够实现边合成边播放的效果，显著降低了首字节延迟。音频数据采用 base64 编码传输，前端接收后进行解码并通过 Web Audio API 进行播放。

**音频评分分析功能**是本项目的特色功能之一，基于深度学习技术对用户的语音进行多维度专业评分。系统采用 CAMPPlus 神经网络模型，通过提取音频的 MFCC（梅尔频率倒谱系数）特征，对用户的音准、节奏、音色、情感和完整性进行量化评估。评分结果以百分制呈现，每个维度都有独立的分数和详细的评价说明。该功能适用于歌唱评测、演讲训练、发音纠正等多种应用场景。

**Live2D 角色展示功能**使用 Live2D Web SDK 在浏览器中渲染动态的二次元角色模型。角色能够根据用户的交互行为做出相应的表情变化和肢体动作，增加了系统的趣味性和沉浸感。角色模型支持自定义配置，可以根据需求替换为不同的模型文件。系统还实现了角色与对话内容的联动，角色能够根据回复文本的情绪自动调整表情和口型。

### 1.3 技术架构概览

项目整体采用分层架构设计，从底层到顶层依次为数据存储层、服务层、通信层和展示层。这种分层设计使得各层之间的耦合度降低，便于独立开发、测试和部署。数据存储层主要负责临时文件和模型权重的管理，使用文件系统和环境变量进行配置。服务层是整个系统的核心，包含语音识别、语音合成、音频评分和智能对话等多个微服务模块。通信层负责前端与后端之间的数据交换，同时支持 HTTP 轮询和 WebSocket 实时通信两种方式。展示层即为前端界面，负责用户交互界面的渲染和事件处理。

```
┌─────────────────────────────────────────────────────────────┐
│                        展示层                                │
│              mao-demo.html / chat_interface.html             │
│                  (HTML5 + CSS3 + JavaScript)                 │
├─────────────────────────────────────────────────────────────┤
│                        通信层                                │
│         HTTP REST API + WebSocket 实时通信                   │
├──────────────────────┬──────────────────────────────────────┤
│                      │              服务层                   │
│      HTTP 服务       │  ┌────────────────────────────────┐   │
│      (8000/8001)     │  │  语音识别服务 (8002/8005)      │   │
│                      │  │  ASR + 音频评分 API            │   │
│                      │  ├────────────────────────────────┤   │
│                      │  │  智能对话服务 (8003)           │   │
│                      │  │  Qwen LLM API                  │   │
│                      │  ├────────────────────────────────┤   │
│                      │  │  语音合成服务 (8004)           │   │
│                      │  │  TTS WebSocket Server          │   │
│                      │  └────────────────────────────────┘   │
├──────────────────────┴──────────────────────────────────────┤
│                      数据层                                  │
│           文件系统 + DashScope 云服务 API                    │
└─────────────────────────────────────────────────────────────┘
```

## 二、项目结构

### 2.1 目录组织架构

项目的目录结构按照功能模块进行划分，主目录为 `oh-my-live2d-main`，其下包含前端资源文件、后端服务代码、模型文件和构建产物等多个子目录。根目录下的 `.venv` 目录是 Python 虚拟环境，包含了项目运行所需的所有依赖包。这种组织方式既保证了开发时的代码可读性，又满足了生产部署时的文件完整性需求。

```
oh-my-live2d-main/
├── .venv/                          # Python 虚拟环境
│   └── Lib/site-packages/          # 已安装的依赖包
├── oh-my-live2d-main/              # 项目主目录
│   ├── backend/                    # 后端服务代码
│   │   ├── __pycache__/            # Python 字节码缓存
│   │   ├── .env                    # 环境变量配置
│   │   ├── .env.example            # 环境变量示例
│   │   ├── asr_server.py           # 语音识别和音频评分服务
│   │   ├── main.py                 # 主服务入口
│   │   ├── qwen_chat_server.py     # 智能对话服务
│   │   ├── tts_ws_server.py        # 语音合成 WebSocket 服务
│   │   ├── start_all_servers.bat   # 批量启动服务脚本
│   │   ├── start_cam_s_server.bat  # 启动 CAM-S 服务脚本
│   │   └── start_server.bat        # 启动主服务脚本
│   ├── build/                      # PyInstaller 构建产物
│   │   └── VoiceScorer/            # 音频评分桌面应用
│   ├── dist/                       # 分发包目录
│   │   └── VoiceScorer/            # 可执行的音频评分应用
│   ├── CAM_S.py                    # 音频评分核心模型
│   ├── MFCCnew.py                  # MFCC 特征提取模块
│   ├── VAL_WIN.py                  # 音频窗口处理模块
│   ├── VoiceScorer.spec            # PyInstaller 配置文件
│   ├── app_gradio.py               # Gradio 界面应用
│   └── chat_interface.html         # 聊天界面前端
├── .gitignore                      # Git 忽略配置
└── README.md                       # 项目说明文档
```

### 2.2 前端文件说明

前端文件位于 `oh-my-live2d-main` 目录下，包含 HTML 页面文件和相关资源。目前主要的前端文件是 `chat_interface.html`，这是一个功能完整的聊天界面，集成了语音录制、音频评分、消息展示和 Live2D 角色展示等功能。文件使用原生 JavaScript 编写，不依赖任何前端框架，保持了代码的轻量性和可维护性。

**chat_interface.html** 是系统的核心前端文件，提供了完整的用户交互界面。文件采用单页面应用（SPA）设计，所有功能都在一个页面中完成。页面的主要布局分为左右两个区域：左侧是 Live2D 角色展示区域，右侧是聊天消息和输入控制区域。页面使用了现代化的 CSS 设计风格，包括毛玻璃效果背景、渐变色按钮、圆角卡片等视觉元素。JavaScript 代码实现了 WebSocket 客户端、音频录制、音频播放、消息处理等核心功能。

### 2.3 后端服务说明

后端服务代码位于 `backend` 目录下，使用 Python FastAPI 框架开发。各服务模块采用统一的设计模式，都实现了健康检查接口和 CORS 跨域支持，能够与前端进行无缝对接。

**asr_server.py** 是语音识别和音频评分的核心服务模块。该文件实现了三个主要功能：一是语音识别功能，调用 DashScope ASR API 将音频转换为文字；二是音频评分功能，使用 CAM-S 模型对音频进行专业评分；三是语音识别结果的轮询查询机制，支持前端通过 GET 请求查询识别结果。服务的默认监听端口为 8005。

**qwen_chat_server.py** 是智能对话服务模块，封装了通义千问大语言模型的 API 调用。该服务接收前端发送的对话请求，调用云端模型生成回复，并返回给前端进行展示。服务支持上下文记忆，能够在多轮对话中保持对话连贯性。服务的默认监听端口为 8003。

**tts_ws_server.py** 是语音合成 WebSocket 服务模块，提供实时语音合成能力。该服务使用 WebSocket 协议与前端建立持久连接，支持流式传输音频数据。服务支持多种语音角色和语言类型的配置，能够根据客户端的请求动态调整合成参数。服务的默认监听端口为 8004。

**main.py** 是一个综合性的后端服务示例，整合了语音识别、语音合成和 TTS 功能。该文件展示了如何将多个服务整合到同一个 FastAPI 应用中，适合作为功能扩展的参考模板。

### 2.4 核心模块说明

**CAM_S.py** 是音频评分系统的核心实现，包含了 CAMPPlus 神经网络模型的定义和推理逻辑。该模块使用 PyTorch 框架实现，支持加载预训练的模型权重进行推理。模块的主要功能是接收音频特征，输出多维度的评分结果。

**MFCCnew.py** 实现了 MFCC（梅尔频率倒谱系数）特征提取功能。MFCC 是语音信号处理中广泛使用的特征表示方法，能够有效描述语音的频谱特性。该模块使用 librosa 库实现，提供了灵活的配置参数。

**VAL_WIN.py** 提供了音频数据的窗口处理功能，用于将连续的音频信号分割成固定长度的片段，以便进行特征提取和模型推理。该模块支持重叠窗口和非重叠窗口两种模式。

**app_gradio.py** 是一个基于 Gradio 框架的音频评分界面应用。Gradio 提供了简洁的 Web 界面开发方式，适合快速构建机器学习模型的演示界面。该文件可以作为开发独立音频评分应用的参考。

## 三、技术架构详解

### 3.1 前端技术栈

前端技术栈采用原生 Web 技术实现，不依赖任何第三方前端框架，保证了代码的轻量性和加载速度。

**HTML5** 提供了页面的结构基础，使用语义化的标签组织页面内容。页面中使用了 `<audio>` 元素进行音频播放，使用 `<video>` 元素进行视频播放（如需要），使用 `<canvas>` 元素进行音频可视化展示。

**CSS3** 负责页面的视觉设计和动画效果。页面大量使用了 CSS 变量进行主题配色管理，使用 Flexbox 和 Grid 进行页面布局，使用 CSS 动画和过渡效果增强交互反馈。主要的视觉特性包括：毛玻璃效果的背景模糊、渐变色按钮和背景、圆角卡片设计、平滑的过渡动画等。

**JavaScript (ES6+)** 实现了前端的所有交互逻辑。代码采用了模块化的组织方式，将不同功能的代码分离到不同的函数和对象中。主要的技术实现包括：使用 Web Audio API 进行音频录制和播放、使用 WebSocket API 进行实时通信、使用 fetch API 进行 HTTP 请求、使用 MediaRecorder API 进行录音等。

### 3.2 后端技术栈

后端技术栈基于 Python 生态系统，使用了多个成熟的库和框架。

**FastAPI** 是一个高性能的 Python Web 框架，用于构建 API 服务。FastAPI 提供了自动生成 API 文档、请求参数验证、依赖注入等功能，大大简化了 API 开发的复杂度。项目中使用 FastAPI 构建了 RESTful API 和 WebSocket 端点。

**Dashscope** 是阿里云百炼平台的 Python SDK，提供了语音识别（ASR）和语音合成（TTS）的 API 封装。项目中使用 Dashscope 进行语音相关的云端处理，利用阿里云强大的 AI 能力提供高质量的语音服务。

**Librosa** 是一个用于音频和音乐分析的 Python 库，提供了丰富的音频处理功能。项目中使用 Librosa 进行 MFCC 特征提取，支持灵活的参数配置。

**PyTorch** 是一个开源的深度学习框架，用于加载和运行 CAM-S 音频评分模型。项目中使用 PyTorch 进行模型推理，计算音频的多维度评分。

### 3.3 通信协议设计

系统支持两种通信方式：HTTP REST API 和 WebSocket 实时通信。不同的功能根据其特点选择合适的通信方式。

**HTTP REST API** 适用于单次请求响式的场景，如健康检查、音频上传、评分查询等。API 设计遵循 RESTful 规范，使用 HTTP 方法表示操作类型，使用 JSON 格式传输数据。所有的 API 端点都实现了 CORS 跨域支持，允许前端跨域访问。

**WebSocket** 适用于需要实时双向通信的场景，如语音合成的流式传输。WebSocket 建立了客户端与服务器之间的持久连接，双方可以随时发送消息，不需要每次都建立新的连接。TTS 服务使用 WebSocket 协议，实现了边合成边播放的效果。

### 3.4 数据格式规范

系统定义了统一的数据格式规范，确保前后端数据交换的一致性。

**请求数据格式**：所有 API 请求均使用 JSON 格式传递参数。音频数据采用 base64 编码后传输，编码后的字符串可以直接在 JSON 中表示。音频格式约定为 PCM16LE、单声道、16kHz 采样率。

**响应数据格式**：API 响应统一使用 JSON 格式，包含状态码、消息和数据三个部分。成功的响应包含 `success: true` 字段，失败的响应包含 `error` 字段描述错误信息。音频评分响应还包含具体的评分维度数据。

**WebSocket 消息格式**：WebSocket 消息使用 JSON 格式，每条消息都包含 `type` 字段表示消息类型。主要的消息类型包括：`input_text_buffer.append`（追加文本）、`input_text_buffer.commit`（提交文本）、`session.finish`（结束会话）、`response.audio.delta`（音频数据）、`response.audio.meta`（音频元数据）、`error`（错误消息）等。

## 四、功能模块详解

### 4.1 语音识别模块

语音识别模块负责将用户的语音输入转换为文本，是整个语音交互系统的入口环节。

**音频录制流程**：前端使用 Web Audio API 创建音频上下文，通过 `navigator.mediaDevices.getUserMedia` 获取麦克风权限，使用 MediaRecorder 进行录音。录音过程中，音频数据被实时收集并转换为 PCM 格式。录制完成后，音频数据被转换为 base64 编码的字符串，通过 HTTP POST 请求发送给后端服务。

**ASR 服务处理流程**：后端接收到音频数据后，首先进行 base64 解码，将编码后的字符串还原为原始的 PCM 字节流。然后将 PCM 数据写入临时 WAV 文件，作为 DashScope ASR API 的输入。API 返回识别结果后，后端将结果进行格式化处理，并通过轮询机制或直接返回的方式将结果发送给前端。

**语音识别 API 接口**：

```
POST /api/audio/transcribe
  功能：提交音频进行语音识别
  请求体：{ audio_data: string, audio_format?: string }
  响应：{ success: boolean, text?: string, error?: string }

GET /api/voice/text
  功能：查询语音识别结果
  响应：{ text: string, has_result: boolean, error?: string }
```

### 4.2 智能对话模块

智能对话模块负责理解用户输入并生成智能回复，是系统的核心交互功能。

**对话处理流程**：前端将用户输入的文本通过 HTTP 请求发送给后端服务。后端接收到请求后，首先进行文本预处理，包括去除多余空白、敏感词过滤等。然后将处理后的文本与对话历史一起发送给通义千问大语言模型 API。API 返回生成的回复文本后，后端进行后处理（包括格式化、敏感词过滤等），然后将结果返回给前端。

**上下文管理机制**：系统实现了简单的上下文管理机制，将对话历史保存在内存中。每次新的请求都会将历史对话作为上下文发送给模型，使模型能够理解对话的连贯性。上下文包含用户输入和模型回复的交替序列，最多保留最近 N 轮对话。

**对话 API 接口**：

```
POST /api/chat
  功能：发送对话请求
  请求体：{ message: string, history?: array }
  响应：{ success: boolean, reply?: string, error?: string }
```

### 4.3 语音合成模块

语音合成模块负责将文本转换为自然语音进行播放，是语音交互系统的输出环节。

**WebSocket 连接流程**：前端在用户发起对话时建立与 TTS 服务的 WebSocket 连接。连接建立后，双方进行握手认证（如果需要），然后进入消息交换阶段。前端通过 WebSocket 发送文本消息，服务端接收后调用 DashScope TTS API 进行语音合成。合成的音频数据通过 WebSocket 流式传输回前端，前端接收后进行解码和播放。

**流式音频处理**：TTS 服务支持流式音频传输，合成一段音频后立即发送给前端，而不是等待全部合成完成。这种方式显著降低了首字节延迟，用户可以更快地听到语音输出。音频数据采用 base64 编码传输，每条消息包含一个音频片段。

**音频播放机制**：前端接收到音频数据后，首先进行 base64 解码，将编码后的字符串转换为 Uint8Array。然后使用 Web Audio API 创建音频缓冲区，通过 AudioContext 的 `decodeAudioData` 方法进行解码，最后使用 `AudioBufferSourceNode` 进行播放。为了实现无缝播放，还实现了音频队列管理机制。

**TTS WebSocket 接口**：

```
WS /ws/tts
  功能：语音合成 WebSocket 端点
  URL 参数：model?, voice?, language_type?
  客户端消息：
    - { type: "input_text_buffer.append", text: string }
    - { type: "input_text_buffer.commit" }
    - { type: "session.finish" }
  服务端消息：
    - { type: "session.ready", model, voice, language_type }
    - { type: "response.audio.delta", delta: string }
    - { type: "response.audio.meta", meta: { url: string } }
    - { type: "error", error: string }
```

### 4.4 音频评分模块

音频评分模块是本项目的特色功能，对用户的语音进行多维度的专业评分。

**评分维度定义**：系统定义了五个评分维度，分别是音准（Pitch）、节奏（Rhythm）、音色（Timbre）、情感（Emotion）和完整性（Integrity）。每个维度采用百分制评分，分数越高表示该维度的表现越好。系统还计算所有维度的加权平均值作为总体评分。

**特征提取流程**：音频数据首先被转换为 WAV 格式，然后使用 Librosa 库提取 MFCC 特征。MFCC 特征能够有效描述语音的频谱特性，是语音分析中广泛使用的特征表示。特征提取过程中可以配置采样率、帧长、帧移等参数。

**模型推理流程**：提取的 MFCC 特征被转换为 PyTorch 张量，作为 CAMPPlus 模型的输入。模型接收固定维度的特征向量，输出五个维度的评分。模型使用预训练的权重文件进行推理，权重文件按声部类型分类存放。

**评分 API 接口**：

```
POST /api/audio/score
  功能：对音频进行专业评分
  请求体：{ audio_data: string, voice_type: string }
  响应：{
    success: boolean,
    overall: number,
    scores: array,
    voice_type: string,
    original_sr: number,
    target_sr: number,
    error?: string
  }
```

### 4.5 Live2D 展示模块

Live2D 展示模块负责在网页中渲染动态的二次元角色模型。

**模型加载流程**：页面加载时，脚本会初始化 Live2D Web SDK，并加载指定路径的模型文件。模型文件包括模型描述文件（.model3.json）、纹理文件（.png）、物理文件（.physics3.json）等。加载完成后，模型会被渲染到页面指定的 canvas 元素中。

**交互响应机制**：模型支持用户交互，包括点击、拖拽等操作。用户点击模型区域时，模型会做出相应的表情变化或动作响应。模型还可以根据对话内容自动调整表情，实现口型同步和情绪表达。

## 五、API 接口文档

### 5.1 通用说明

所有 API 端点均遵循以下规范：请求和响应均使用 JSON 格式；字符编码为 UTF-8；日期时间格式为 ISO 8601；所有时间戳为 Unix 时间戳（秒或毫秒）。

**CORS 配置**：服务配置了 CORS 中间件，允许来自 localhost:8000 和 localhost:8001 的跨域请求。这意味着前端页面可以直接从这两个端口调用 API，无需额外的代理配置。

**错误响应格式**：所有 API 在发生错误时都会返回标准错误响应，格式如下：

```json
{
  "success": false,
  "error": "错误描述信息"
}
```

### 5.2 ASR 服务接口

ASR 服务（asr_server.py）提供了语音识别和音频评分功能。

**健康检查接口**

```
GET /health
  功能：检查服务健康状态
  响应：{ status: "ok", service: "asr" }
```

**音频转写接口**

```
POST /api/audio/transcribe
  功能：将音频转换为文字
  请求头：Content-Type: application/json
  请求体：
  {
    "audio_data": "base64编码的音频数据",
    "audio_format": "pcm16le_16k_mono"
  }
  响应：
  {
    "success": true,
    "text": "识别出的文本内容"
  }
```

**音频评分接口**

```
POST /api/audio/score
  功能：对音频进行专业评分
  请求体：
  {
    "audio_data": "base64编码的音频数据",
    "voice_type": " soprano | alto | tenor | bass "
  }
  响应：
  {
    "success": true,
    "message": "音频评分完成",
    "overall": 85.5,
    "scores": [
      { "technique": "音准", "score": 88 },
      { "technique": "节奏", "score": 82 },
      { "technique": "音色", "score": 85 },
      { "technique": "情感", "score": 87 },
      { "technique": "完整性", "score": 85 }
    ],
    "voice_type": "soprano",
    "original_sr": 16000,
    "target_sr": 16000
  }
```

### 5.3 对话服务接口

对话服务（qwen_chat_server.py）提供了智能对话功能。

**发送对话请求接口**

```
POST /api/chat
  功能：发送对话请求，获取智能回复
  请求体：
  {
    "message": "用户输入的文本",
    "history": [
      { "role": "user", "content": "上一轮用户输入" },
      { "role": "assistant", "content": "上一轮模型回复" }
    ]
  }
  响应：
  {
    "success": true,
    "reply": "模型生成的回复文本"
  }
```

### 5.4 TTS 服务接口

TTS 服务（tts_ws_server.py）使用 WebSocket 提供语音合成功能。

**WebSocket 端点**

```
WS /ws/tts?model=&voice=&language_type=
  功能：建立语音合成 WebSocket 连接
  URL 参数：
    - model：语音合成模型（默认：cosyvoice-v1）
    - voice：语音角色（默认：maoxiaoxiao）
    - language_type：语言类型（默认：chinese）
  示例：ws://localhost:8004/ws/tts?voice=william&language_type=english
```

**消息格式**

客户端发送消息：

```json
{ "type": "input_text_buffer.append", "text": "要合成的文本" }
{ "type": "input_text_buffer.commit" }
{ "type": "session.finish" }
```

服务端返回消息：

```json
{ "type": "session.ready", "model": "cosyvoice-v1", "voice": "maoxiaoxiao", "language_type": "chinese" }
{ "type": "response.audio.delta", "delta": "base64编码的音频数据" }
{ "type": "response.audio.meta", "meta": { "url": "完整音频文件的下载URL" } }
{ "type": "error", "error": "错误描述" }
```

### 5.5 主服务接口

主服务（main.py）整合了多个功能模块。

**语音识别接口**

```
POST /api/voice/start
  功能：开始语音识别
  请求体：{ audio_base64: string }
  响应：{ success: boolean }

GET /api/voice/text
  功能：获取识别结果
  响应：{ text: string, has_result: boolean, error?: string }

POST /api/voice/clear
  功能：清除识别结果
  响应：{ success: boolean }
```

**语音合成接口**

```
POST /api/tts
  功能：合成语音
  请求体：{ text: string, voice_type?: string }
  响应：{ success: boolean, audio_base64?: string, error?: string }
```

## 六、配置说明

### 6.1 环境变量配置

后端服务使用 `.env` 文件进行环境变量配置，文件位于 `backend` 目录下。首次部署时，可以参考 `.env.example` 文件创建实际的配置文件。

**必需配置项**

```
DASHSCOPE_API_KEY=your_api_key_here
  说明：阿里云百炼平台的 API Key，用于调用 ASR、TTS 和 LLM 服务
  获取方式：登录阿里云控制台，在百炼平台创建 API Key
```

**可选配置项**

```
# ASR 服务配置
ASR_HOST=0.0.0.0
  说明：ASR 服务绑定的地址
ASR_PORT=8005
  说明：ASR 服务监听的端口号
DASHSCOPE_ASR_MODEL=paraformer-v1
  说明：语音识别使用的模型名称

# 对话服务配置
CHAT_HOST=0.0.0.0
  说明：对话服务绑定的地址
CHAT_PORT=8003
  说明：对话服务监听的端口号

# TTS 服务配置
TTS_HOST=0.0.0.0
  说明：TTS 服务绑定的地址
TTS_PORT=8004
  说明：TTS 服务监听的端口号
DEFAULT_MODEL=cosyvoice-v1
  说明：默认使用的 TTS 模型
DEFAULT_VOICE=maoxiaoxiao
  说明：默认使用的语音角色
DEFAULT_LANG=chinese
  说明：默认使用的语言类型
```

### 6.2 DashScope API 配置

DashScope 是阿里云百炼平台的统一服务入口，提供了语音识别、语音合成和大语言模型等 AI 能力。项目中使用 DashScope 的以下功能：

**语音识别（ASR）**：使用 paraformer-v1 模型，支持中英文混合识别，能够处理音频文件或音频流。

**语音合成（TTS）**：使用 cosyvoice-v1 模型，支持多种语音角色和语言类型，提供流式音频输出。

**大语言模型（LLM）**：使用通义千问系列模型，提供智能对话能力。

### 6.3 CAM-S 模型配置

音频评分系统使用 CAMPPlus 神经网络模型，需要配置模型权重文件的路径。模型权重按声部类型分类存放，不同的声部类型使用不同的权重文件：

```
# 模型权重目录结构
CAM_S.py 所在目录/
├── checkpoints/
│   ├── soprano/          # 女高音权重
│   │   └── checkpoint_xx.pth
│   ├── alto/             # 女中音权重
│   ├── tenor/            # 男高音权重
│   └── bass/             # 男低音权重
```

权重文件的加载在 `CAM_S.py` 中通过 `DEFAULT_WEIGHTS` 字典进行配置。当前端请求音频评分时，需要指定 `voice_type` 参数，后端会根据该参数选择对应的模型权重文件。

## 七、部署和运行指南

### 7.1 环境准备

在部署项目之前，需要确保目标机器满足以下环境要求：

**Python 环境**：项目需要 Python 3.8 或更高版本，建议使用 Python 3.10 或 3.11 以获得最佳兼容性。可以使用 `python --version` 命令检查当前 Python 版本。

**虚拟环境**：建议使用虚拟环境隔离项目依赖。可以使用 venv 或 conda 创建虚拟环境。创建虚拟环境的命令为：`python -m venv .venv`。

**依赖安装**：激活虚拟环境后，使用 pip 安装项目依赖：

```bash
pip install -r requirements.txt
```

主要依赖包括：fastapi、uvicorn、python-dotenv、dashscope、librosa、torch、numpy、pandas 等。

### 7.2 服务启动方式

项目提供了多种服务启动方式，根据需求选择合适的启动方式。

**方式一：批量启动所有服务**

使用项目提供的批处理脚本一次性启动所有后端服务：

```bash
cd backend
start_all_servers.bat
```

该脚本会依次启动 ASR 服务（8005）、对话服务（8003）和 TTS 服务（8004）。

**方式二：分别启动各个服务**

分别启动各个后端服务，便于调试和排查问题：

```bash
# 启动 ASR 和音频评分服务
cd backend
uvicorn asr_server:app --host 0.0.0.0 --port 8005 --reload

# 启动对话服务
uvicorn qwen_chat_server:app --host 0.0.0.0 --port 8003 --reload

# 启动 TTS 服务
uvicorn tts_ws_server:app --host 0.0.0.0 --port 8004 --reload
```

**方式三：启动前端 HTTP 服务**

前端页面需要通过 HTTP 服务访问，不能直接打开本地文件（否则会有跨域和安全限制）。

```bash
# 启动 mao-demo.html 服务（端口 8000）
cd oh-my-live2d-main
python -m http.server 8000

# 启动 chat_interface.html 服务（端口 8001）
python -m http.server 8001
```

### 7.3 访问地址

服务启动后，可以通过以下地址访问各功能页面：

| 服务 | 地址 | 说明 |
|------|------|------|
| mao-demo.html | http://localhost:8000/mao-demo.html | Live2D 角色演示页面 |
| chat_interface.html | http://localhost:8001/chat_interface.html | 完整聊天功能页面 |
| ASR 健康检查 | http://localhost:8005/health | ASR 服务状态 |
| 对话服务 | http://localhost:8003/docs | Swagger API 文档 |

### 7.4 Docker 部署（可选）

如果使用 Docker 部署，可以参考以下 Dockerfile：

```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000 8001 8002 8003 8004 8005

CMD ["python", "-m", "http.server", "8000", "--directory", "/app/oh-my-live2d-main"]
```

## 八、常见问题解答

### 8.1 端口冲突问题

**问题表现**：启动服务时报错 `Address already in use`，提示端口已被占用。

**解决方法**：首先使用以下命令查找占用端口的进程：

```bash
netstat -ano | findstr :端口号
```

然后根据返回的 PID 使用以下命令终止进程：

```bash
taskkill /F /PID 进程PID
```

如果不想终止占用端口的进程，也可以修改服务配置文件，将服务端口改为其他未被占用的端口。

### 8.2 依赖库缺失问题

**问题表现**：启动服务时报错 `ModuleNotFoundError`，提示缺少某个 Python 库。

**解决方法**：使用 pip 安装缺失的库：

```bash
pip install 库名
```

如果遇到安装失败的情况，可能是由于系统缺少编译依赖。对于 librosa 和 torch 等库，建议使用预编译的 wheel 包安装：

```bash
pip install --only-binary :all: librosa
```

### 8.3 WebSocket 连接失败

**问题表现**：前端连接 TTS 服务时显示 `WebSocket connection failed` 或收到 404 错误。

**解决方法**：首先确认 TTS 服务已经启动并且端口正确。然后检查是否是 uvicorn 缺少 WebSocket 依赖：

```bash
pip install uvicorn[standard]
```

或者单独安装 WebSocket 依赖：

```bash
pip install websockets wsproto
```

最后确认启动命令使用了正确的模块名和入口点：

```bash
uvicorn tts_ws_server:app --host 0.0.0.0 --port 8004
```

### 8.4 音频评分功能不可用

**问题表现**：调用音频评分 API 时返回 "音频评分功能不可用，缺少必要的库"。

**解决方法**：检查是否安装了音频评分所需的依赖库：

```bash
pip install torch librosa numpy pandas tensorboardX
```

如果库已安装但仍然报错，可能是 CAM_S.py 模块导入失败。检查控制台输出的具体错误信息，可能是模型权重文件路径配置错误或权重文件缺失。

### 8.5 语音识别结果为空

**问题表现**：上传音频进行语音识别后，返回的文本为空。

**解决方法**：首先检查音频数据是否正确上传，音频格式是否为 PCM16LE、16kHz、单声道。其次检查 DashScope API Key 是否有效。最后查看后端日志，查看是否有 API 调用错误信息。

### 8.6 页面无法加载

**问题表现**：打开前端页面时显示空白或加载失败。

**解决方法**：确认前端 HTTP 服务已经启动且端口正确。打开浏览器开发者工具，查看控制台是否有错误信息。常见原因包括：Live2D 模型文件路径错误、JavaScript 语法错误、网络请求被拦截等。

## 九、开发指南

### 9.1 添加新的语音角色

要添加新的 TTS 语音角色，需要在 `tts_ws_server.py` 中修改配置：

```python
# 在 DEFAULT_VOICES 中添加新的语音角色
DEFAULT_VOICES = {
    "maoxiaoxiao": "毛晓萱",
    "xiaoxiao": "晓晓",
    "cherry": "Cherry",
    "william": "William",
    # 添加新角色...
}
```

前端页面需要同步更新语音角色选择的下拉列表，添加新角色的选项。

### 9.2 添加新的评分维度

要添加新的音频评分维度，需要修改以下文件：

1. **CAM_S.py**：在模型输出中添加新的评分维度，更新模型的输出层节点数。

2. **asr_server.py**：在 `TECH_NAMES` 列表中添加新的维度名称，更新评分结果的返回格式。

3. **前端页面**：在评分结果显示区域添加新的维度展示元素，更新评分展示的 UI 代码。

### 9.3 自定义对话模型

要使用其他的大语言模型替代通义千问，需要修改 `qwen_chat_server.py` 中的 API 调用部分。例如，替换为 OpenAI API：

```python
import openai

openai.api_key = "your_api_key"

def chat_with_llm(message: str, history: list) -> str:
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=history + [{"role": "user", "content": message}]
    )
    return response.choices[0].message.content
```

### 9.4 添加新的前端页面

要添加新的前端页面，建议按照以下步骤进行：

1. 在 `oh-my-live2d-main` 目录下创建新的 HTML 文件。

2. 复制 `chat_interface.html` 的基本结构和样式代码作为起点。

3. 根据新页面的功能需求修改 HTML 结构和 JavaScript 代码。

4. 启动新的 HTTP 服务或修改现有服务以包含新页面。

5. 在项目的导航或入口页面添加指向新页面的链接。

## 十、扩展功能建议

### 10.1 多人实时对话

当前系统只支持单用户对话，可以扩展为支持多用户实时对话功能。需要引入 WebSocket 服务器实现用户之间的消息广播，并添加用户管理和房间管理功能。

### 10.2 语音情绪识别

可以集成语音情绪识别功能，分析用户语音中的情绪特征，使回复更加贴合用户当前的情绪状态。常用的情绪识别模型包括 Wav2Vec2、HuBERT 等。

### 10.3 语音克隆

利用 DashScope 的声音克隆功能，可以让用户上传自己的声音样本，然后使用克隆的声音进行 TTS 播放。这需要额外的语音样本采集和处理流程。

### 10.4 对话历史持久化

当前对话历史只保存在内存中，页面刷新后会丢失。可以添加数据库存储功能，将对话历史保存到本地数据库或云端存储，实现跨会话的历史记录查询功能。

### 10.5 音频可视化

可以添加音频波形可视化功能，在录音和播放时实时显示音频波形。这需要使用 Web Audio API 的 AnalyserNode 获取频率数据，然后使用 Canvas 进行绘制。